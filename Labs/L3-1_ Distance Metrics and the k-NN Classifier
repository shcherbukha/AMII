{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fhTV2vYIvwtbWaLNJMFb6sPWJuZnEsI9","timestamp":1665887636449},{"file_id":"117FuonQ6JoUfVCgawHcN0T6owbVHy8rB","timestamp":1605850449615}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["###Disclaimer\n","The information contained in this notebook and any accompanying files are proprietary and is confidential to the participants of the Machine Learning Technician program and should not be copied, distributed or reproduced in whole or in part, nor passed to any third party without written permission from the Alberta Machine Intelligence Institute, Amii."],"metadata":{"id":"tPm2tPeAG28d"}},{"cell_type":"markdown","metadata":{"id":"0Iyk9QUYB3rJ"},"source":["# L3-1: Distance Metrics and the *k*-NN Classifier"]},{"cell_type":"markdown","source":["Let's import some packages:"],"metadata":{"id":"XEw-QjZRvtoy"}},{"cell_type":"code","metadata":{"id":"c2VB7WarCCI4","executionInfo":{"status":"ok","timestamp":1665886876730,"user_tz":420,"elapsed":276,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["import numpy as np"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Let's talk about distances first:"],"metadata":{"id":"MkrKG0hexRt_"}},{"cell_type":"markdown","metadata":{"id":"-s5x9192B967"},"source":["## Distance Measures"]},{"cell_type":"markdown","source":["Now, let's review how we can represent matrices with NumPy. Here, `M` is a matrix:"],"metadata":{"id":"_JvszyyWv0Fj"}},{"cell_type":"code","metadata":{"id":"jS4rogTACm4A","executionInfo":{"status":"ok","timestamp":1665886879474,"user_tz":420,"elapsed":280,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["M = np.array([[1, 2],\n","              [4, 3],\n","              [5, 6]])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vsTzrXXDGyn","executionInfo":{"status":"ok","timestamp":1665886882848,"user_tz":420,"elapsed":276,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"4a1bdc63-3d4c-4c6c-dd7f-0ddc465d686e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(M.shape)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["(3, 2)"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"6DUDOga0CyWn","executionInfo":{"status":"ok","timestamp":1665886885567,"user_tz":420,"elapsed":239,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"ca0495bd-9862-42a6-875f-73e9e86ca56c","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["display(M)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[1, 2],\n","       [4, 3],\n","       [5, 6]])"]},"metadata":{}}]},{"cell_type":"markdown","source":["...and `v` is the row index 0 (first row) of `M`, which is a row vector. As we said, we usually use column vectors, here though, because we extract the vector from matrix `M`, its going to be a 1D vector (again, we usually represent vectors as 2D) and hence it's not really a row or column vector, it's better called just 'vector':"],"metadata":{"id":"THegJQYFwVpO"}},{"cell_type":"code","metadata":{"id":"eBDFnWVHC6Qt","executionInfo":{"status":"ok","timestamp":1665887118467,"user_tz":420,"elapsed":240,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["v = M[0, :]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8im4u2n9DD24","executionInfo":{"status":"ok","timestamp":1665887120758,"user_tz":420,"elapsed":274,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"a5079d3f-bdb8-4ede-bafb-0d07f03d3da0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(v)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([1, 2])"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"jYzyEijcDOam","executionInfo":{"status":"ok","timestamp":1665887129521,"user_tz":420,"elapsed":235,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"d8912640-0260-4bca-d625-b19ad9c7fc2c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(v.shape)"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["(2,)"]},"metadata":{}}]},{"cell_type":"markdown","source":["`n` is the size of our vector:"],"metadata":{"id":"y5LdOaxHwXoU"}},{"cell_type":"code","metadata":{"id":"bY5p2AJ1DRp3","executionInfo":{"status":"ok","timestamp":1665887290381,"user_tz":420,"elapsed":3,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["n = v.shape[0]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uy7zNm7DXei","executionInfo":{"status":"ok","timestamp":1665887291630,"user_tz":420,"elapsed":244,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"c585caf4-1cd8-4e67-cd79-c91b9760c637","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(n)"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["2"]},"metadata":{}}]},{"cell_type":"markdown","source":["You can subtract vectros from each other:"],"metadata":{"id":"V_MTwpxKwjOw"}},{"cell_type":"code","metadata":{"id":"ixJIjpNgFzrn","executionInfo":{"status":"ok","timestamp":1665887297976,"user_tz":420,"elapsed":255,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"ebefe273-08b9-4510-850f-ff46be018595","colab":{"base_uri":"https://localhost:8080/"}},"source":["M[0, :] - M[1, :]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-3, -1])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Now, let's review the definition of the Euclidian (or $L^2$) distance:"],"metadata":{"id":"nsbZG0G2xAaf"}},{"cell_type":"markdown","metadata":{"id":"Kg0Yove5Gg0Y"},"source":["Euclidian ($L^2$) distance: $$L^2(\\mathbf{a},\\mathbf{b}):=\\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+(a_3-b_3)^2+\\ldots+(a_n-b_n)^2}\\,.$$\n","\n","$L^2$ norm: $$||\\mathbf{v}||_2:=L^2(\\mathbf{v},\\mathbf{v})\\,.$$\n","\n","So:\n","$$L^2(\\mathbf{a},\\mathbf{b})=||\\mathbf{a}-\\mathbf{b}||_2\\,.$$\n","\n","**Remark**: When we use the norm $||\\cdot||$ operator without any subscript we usually mean the $L^2$ norm, i.e., for all $\\mathbf{v}$:\n","$$||\\mathbf{v}||=||\\mathbf{v}||_2\\,.$$"]},{"cell_type":"markdown","source":["Now, let's write a function that calculates the $L^2$ using regular Python loops to go through the elements (dimensions) of points. we need the `math` package for some mathematical functions we are going to use:"],"metadata":{"id":"IRdfSVuexPDJ"}},{"cell_type":"code","metadata":{"id":"uoAitzO1EHoe","executionInfo":{"status":"ok","timestamp":1665887331146,"user_tz":420,"elapsed":257,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["import math"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["...and here's the function:"],"metadata":{"id":"IWQ3_fNrxkE-"}},{"cell_type":"code","metadata":{"id":"bXExzPchCEKf","executionInfo":{"status":"ok","timestamp":1665887333668,"user_tz":420,"elapsed":227,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["def L2_point_for_loop(x1, x2):\n","  sum_of_squares = 0.\n","  v = x1 - x2\n","  for i in range(v.shape[0]):\n","    sum_of_squares += v[i] ** 2\n","  L2 = math.sqrt(sum_of_squares)\n","  return L2"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Go through the function line by line. It is exactly calculating that mathematical formula.\n","\n","Now, let's do some distance calculation with our function:"],"metadata":{"id":"6lI3NjiPxnBg"}},{"cell_type":"code","metadata":{"id":"7enRpwenDwln","executionInfo":{"status":"ok","timestamp":1665887560220,"user_tz":420,"elapsed":253,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["M = np.array([[1, 2],\n","              [9, 8]])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6_mCtKvD6bd","executionInfo":{"status":"ok","timestamp":1665887562500,"user_tz":420,"elapsed":249,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"b9618352-c3fd-4b97-a75b-a008582a51f7","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(L2_point_for_loop(M[0, :], M[1, :]))"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["10.0"]},"metadata":{}}]},{"cell_type":"markdown","source":["Good!\n","\n","You can also use NumPy to do these calculations! The (huge) advantage is that with NumPy you can use 'vectorized' operations rather doing normal loops and increase the efficiency of (i.e., speed up) your calculations by some orders of magnitude. Let's see examples of matrix arithmetic with NumPy:"],"metadata":{"id":"gh33f8axyHO-"}},{"cell_type":"code","metadata":{"id":"2eQemcwXEhsK","executionInfo":{"status":"ok","timestamp":1665887631875,"user_tz":420,"elapsed":256,"user":{"displayName":"Andrii","userId":"13546378101700851762"}},"outputId":"a40c86b9-26e7-4b3f-ec22-7cf36bb474c0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["display(M[0, :] * M[1, :])"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([ 9, 16])"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Scg6yjufFr5N","executionInfo":{"status":"ok","timestamp":1665887634187,"user_tz":420,"elapsed":439,"user":{"displayName":"Andrii","userId":"13546378101700851762"}}},"source":["v = M[0, :] ** 2"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"CH-gbXaqF9bm"},"source":["display(v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orY8FhTWGE_U"},"source":["np.sum(v)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...you can also use the form below, where you use a method of a `ndarray` (NumPy's name for nD NumPy arrays), instead of a function in NumPy:"],"metadata":{"id":"od46GAQwylzc"}},{"cell_type":"code","metadata":{"id":"JaHSgdRjGImp"},"source":["# v.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can use these operations to write a vectorized version of our distance function:"],"metadata":{"id":"1Js5wOzdylVf"}},{"cell_type":"code","metadata":{"id":"OdLYn9LJEYVE"},"source":["def L2_point_vector(x1, x2):\n","  v = x1 - x2\n","  squares = v ** 2\n","  sum_of_squares = np.sum(squares)\n","  L2 = np.sqrt(sum_of_squares)\n","  return L2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can put everything on one line as well:"],"metadata":{"id":"E7DFCqIszCbe"}},{"cell_type":"code","source":["# def l2_point_vector(x1, x2):\n","#   return np.sqrt(np.sum((x1 - x2) ** 2))"],"metadata":{"id":"tE4xV6w0zCgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...and let's use our vectorized $L^2$ distance function for the same calculation:"],"metadata":{"id":"2eCCMIJWzJNE"}},{"cell_type":"code","metadata":{"id":"aBG3oAxjFK_G"},"source":["display(L2_point_vector(M[0, :], M[1, :]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, let's write functions for the Manhattan ($L^1$) distance as well and through defining the $L^1$ norm! First a review of the formulas:"],"metadata":{"id":"FdOF6MAYzRvq"}},{"cell_type":"markdown","metadata":{"id":"-rauFLYGI0Nz"},"source":["$L^1$ norm: $$||\\mathbf{v}||_1:=|v_1| + |v_2| + |v_3| + \\ldots + |v_n|\\,.$$\n","\n","Manhattan ($L^1$) distance: $$L^1(\\mathbf{a},\\mathbf{b}):=||\\mathbf{a}-\\mathbf{b}||_1\\,.$$"]},{"cell_type":"markdown","source":["Now, we can write the function:"],"metadata":{"id":"jafT68evzqoY"}},{"cell_type":"code","metadata":{"id":"Dbfz3lnSGTxl"},"source":["def L1_point_vector(x1, x2):\n","  return np.sum(np.abs(x1 - x2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HclJ3j0cJqhe"},"source":["Note that $L^1$ norm can be rewritten as:\n","$$||\\mathbf{v}||_1:=\\left(|v_1|^1 + |v_2|^1 + |v_3|^! + \\ldots + |v_n|^1\\right)^{1/1}\\,,$$\n","and $L^2$ norm can be written as:\n","$$||\\mathbf{v}||_2:=\\left(|v_1|^2+|v_2|^2+|v_3|^2+\\ldots+|v_n|^2\\right)^{1/2}\\,.$$\n","\n","So, let's review the Minkowski distance and the $L^p$ norm as well:\n","\n","Minkowski ($L^p$) norm: $$||\\mathbf{v}||_p:=\\left(|v_1|^p + |v_2|^p + |v_3|^p + \\ldots + |v_n|^p\\right)^{1 / p}\\,,$$\n","for $p\\geq1$.\n","\n","Minkowski ($L^p$) distance: $$L^p(\\mathbf{a},\\mathbf{b}):=||\\mathbf{a}-\\mathbf{b}||_p\\,.$$"]},{"cell_type":"markdown","source":["..and we can write that in code:"],"metadata":{"id":"zNWT5yAsz9Iv"}},{"cell_type":"code","metadata":{"id":"tj2SKSnEJpus"},"source":["def Lp_point_vector(x1, x2, p):\n","  return (np.sum(np.abs(x1 - x2) ** p)) ** (1 / p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScthnN3D7Yqc"},"source":["Also remember, one special extended case worth mentioning was the $L^\\infty$ norm (and distance, also called Chebyshev distance):\n","$$||\\mathbf{v}||_\\infty=\\lim_{p\\to\\infty}||\\mathbf{v}||_p=\\max_{i=1}^n |v_i|\\,.$$"]},{"cell_type":"markdown","source":["Let's also define the Hamming distance for Boolean or bit vectors:"],"metadata":{"id":"Lma0puOB0JoU"}},{"cell_type":"markdown","metadata":{"id":"slpn2NTbLhLg"},"source":["Hamming distance is the number of bits where the two binary vectors $a$ and $b$ are different, i.e., where one is `True` and the other one is `False` (in matching places). If you convert a Boolean vector to a integer (or floating-point) vector using `.astype('int')` (or `.astype('float')` or ...), `True` values will be converted to `1`s and `False` values will beconverted to `0`. Then, Hamming distance is nothing other than $L^1$ distance on the Boolean vectors converted to integers:"]},{"cell_type":"code","metadata":{"id":"g1EixToxM5tx"},"source":["def hamming_point_vector(x1, x2):\n","  return L1_point_vector(x1.astype('int'), x2.astype('int'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNifwNJzNJbo"},"source":["But what if we wanted to calculate the distance from one point `x_1` to every point stored in matrix `X_2`? "]},{"cell_type":"code","metadata":{"id":"kNlZ2z_1Nmuc"},"source":["v = np.array([1, 2])\n","M = np.array([[1, 4],\n","              [4, 6],\n","              [2, 1]])\n","display(v - M)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you see, we **can** subtract matrices from vectors and the reult is a matrix.\n","\n","We can also sum the vectors in that matrix:"],"metadata":{"id":"DUWQg6OY0Umj"}},{"cell_type":"code","metadata":{"id":"oRNCGJngcw8x"},"source":["np.sum(v - M)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ablbKQuAc10H"},"source":["np.sum(v - M, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, we can calculate distances between a vector and all vectors in that matrix (rows or columns):"],"metadata":{"id":"2catASYg0o4O"}},{"cell_type":"code","metadata":{"id":"mcpDc20aNdhC"},"source":["def Lp(x1, X2, p):\n","  return (np.sum(np.abs(x1 - X2) ** p, axis=1)) ** (1 / p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KiRbeTLSOZtG"},"source":["display(Lp(v, M, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"puxU720JPFCn"},"source":["def L1(x1, X2):\n","  return Lp(x1, X2, 1)\n","\n","def L2(x1, X2):\n","  return Lp(x1, X2, 2)\n","\n","def hamming(x1, X2):\n","  return L1(x1.astype('int'), X2.astype('int'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's do *k*-NN classification using a dataset:"],"metadata":{"id":"w6jzhIJ71G2c"}},{"cell_type":"markdown","metadata":{"id":"fLyr3SP3U04W"},"source":["## Loading California Housing Dataset and Preparation"]},{"cell_type":"code","metadata":{"id":"EzeXXMIUU0bU"},"source":["from sklearn import datasets\n","from sklearn import model_selection\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll use the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset):"],"metadata":{"id":"IcE_bVAC1sN1"}},{"cell_type":"code","metadata":{"id":"7ytSq_-uU-4W"},"source":["data = datasets.fetch_california_housing()\n","X = data.data\n","y_regression = data.target\n","df = pd.DataFrame(np.c_[X, y_regression], \n","                  columns=np.append(data.feature_names, \"MedValue\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3mFXcRvU-7W"},"source":["display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is very obviously a regression dataset. Let's build a classification problem out of this dataset by binning the numerical label, so we can show the usage of a *k*-NN classifier. The threshold values for the bins were determined manually based on the distribution of the data:"],"metadata":{"id":"ggjOo1HE3HNm"}},{"cell_type":"code","metadata":{"id":"0aA641lqWg4H"},"source":["target = df[\"MedValue\"]\n","\n","# Let's use for bins for the 'value class' of houses of California districts:\n","label_categories=[\"low\", \"medium\", \"high\", \"very_high\", \"unknown\"]\n","label_type = pd.CategoricalDtype(categories=label_categories)\n","\n","df[\"ValueClass\"] = \"unknown\"\n","df[\"ValueClass\"] = df[\"ValueClass\"].astype(label_type)\n","\n","# Now, we can create the classification label by binning. \n","# First, we find points that should go inside each class:\n","low_value_rows = (target < 2)\n","medium_value_rows = (target >= 2) & (target < 3)\n","high_value_rows = (target >= 3) & (target < 5)\n","very_high_value_rows = (target >= 5)\n","\n","# ..and create the classification label:\n","df.loc[low_value_rows, \"ValueClass\"] = \"low\"\n","df.loc[medium_value_rows, \"ValueClass\"] = \"medium\"\n","df.loc[high_value_rows, \"ValueClass\"] = \"high\"\n","df.loc[very_high_value_rows, \"ValueClass\"] = \"very_high\"\n","\n","#Finally, let's get rid of the original regression label:\n","df.drop(\"MedValue\", axis=1, inplace=True)\n","\n","display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can check the new classification label column:"],"metadata":{"id":"TU6wHbUl4RvF"}},{"cell_type":"code","metadata":{"id":"_AayjBzeXH8h"},"source":["y = df[\"ValueClass\"].values.astype(\"str\")\n","\n","display(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also split our data into training and test sets. We are basically keeping a part of our data separate from the traning data, so we can use it to evaluate the performance of our *k*-NN classifier. Also, using only 0.5% of the data for testing is not a good idea, however, this is only for demonstration purposes here. We will learn more about data splitting later on in the course:"],"metadata":{"id":"2zbMkaPb4YIM"}},{"cell_type":"code","metadata":{"id":"A4jenNq8ZDmp"},"source":["X_train, X_test, y_train, y_test = \\\n","  model_selection.train_test_split(X, y, test_size=0.005, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can write the k-NN algorithm from scratch!"],"metadata":{"id":"2Ahc2slY-bUQ"}},{"cell_type":"markdown","metadata":{"id":"0b_JKdKFQILh"},"source":["## $k$-NN Algorithm From Scratch\n","\n","Machine learning algorithms have to main functions:\n","- $\\mathbf{w}\\gets train(\\mathbf{X}, \\mathbf{y}, \\mathbf{\\xi})$;\n","- $\\hat{y}\\gets predict(\\mathbf{x'}, \\mathbf{w})$,\n","\n","where $\\mathbf{X}$ are training data points, $\\mathbf{y}$ are training labels, $\\mathbf{\\xi}$ are hyperparameters, $\\mathbf{w}$ is the set of parameters which is the result of training and $\\mathbf{x'}$ is the new point to be predicted (using the weights that were found in training). We can also have a multiple-predicting $predict$ function:\n","- $\\mathbf{\\hat{y}}\\gets predict(\\mathbf{X'}, \\mathbf{w})$.\n","\n","In Python code:\n","\n","`def knn_train(X_train, y_train, k, distance_metric):`\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nothing!\n","\n","$k$-NN is so-called *lazy*! Also:\n","\n","`def knn_predict(x_new, X_train, y_train, k, distance_metric):`\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...\n","\n","With $k$-NN, the hyper-parameters are $(k, \\mathrm{d})$ where $\\mathrm{d}$ is the distance metric. The parameters will be $(\\mathbf{X}, \\mathbf{y}, k, \\mathrm{d})$.\n","\n","Now, let's code $k$-NN from scratch. Remember the procedure for $k$-NN is:\n","1. Calculate distances from new point to be predicted to all training points;\n","2. Sort the training points based on their distance (calculated using the specified distance metric) to the new point to be predicted;\n","3. Find the top $k$ points with the closest distances;\n","4. Find the list of labels in the training labels that correspond with those top $k$ points;\n","5. Find the majority class within that list and return that as the result of prediction.\n","\n","Let's define hyperparameters and the new point so we can do $k$-NN step-by-step first:"]},{"cell_type":"code","metadata":{"id":"_4AGAIGVZ_6B"},"source":["k = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgZ9d0bUZwBn"},"source":["x_new = X_test[0, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYlvv4CBiwJx"},"source":["distance_metric = L2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqE7WxMlzfHb"},"source":["### Step 1"]},{"cell_type":"code","metadata":{"id":"osHp4bQKUcLD"},"source":["distances = distance_metric(x_new, X_train)\n","\n","display(distances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hhWueeHxzv-x"},"source":["### Step 2"]},{"cell_type":"code","metadata":{"id":"UGzPXL56UdzQ"},"source":["sorted_distances = np.sort(distances)\n","\n","display(sorted_distances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WawutqbH0DrH"},"source":["### Step 3"]},{"cell_type":"code","metadata":{"id":"jqZCEZ6ZUkjB"},"source":["top_k_distances = sorted_distances[:k]\n","\n","display(top_k_distances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeA_xFBTzy2-"},"source":["But this won't work! We need indices of where the top $k$ closest points happen, rather than their distances. So:\n","\n","### Step 2 (Fixed)"]},{"cell_type":"code","metadata":{"id":"zzhKeaAKUmjJ"},"source":["sorted_indices_by_distance = np.argsort(distances)\n","\n","display(sorted_indices_by_distance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJnYaoRE0O7H"},"source":["### Step 3 (Fixed)"]},{"cell_type":"code","metadata":{"id":"vQXUHF1va0sr"},"source":["display(sorted_indices_by_distance[:k])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-UBHlXCaetH"},"source":["nearest_k_neighbours_indices = sorted_indices_by_distance[:k]\n","\n","display(nearest_k_neighbours_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_viodjY0Utb"},"source":["### Step 4"]},{"cell_type":"code","metadata":{"id":"17hAT3jFaOrQ"},"source":["nearest_k_neighbours_classes = y_train[nearest_k_neighbours_indices]\n","\n","display(nearest_k_neighbours_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbFRzp0a0aey"},"source":["### Step 5"]},{"cell_type":"code","metadata":{"id":"mLtZyy5ba_js"},"source":["uniques, counts = np.unique(nearest_k_neighbours_classes, return_counts=True)\n","\n","display(uniques, counts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LJYNQKwbcN4"},"source":["display(np.argmax(counts))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnrtxC03bots"},"source":["uniques[np.argmax(counts)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IS66E-zo0e3G"},"source":["### Putting Everything Together"]},{"cell_type":"code","metadata":{"id":"A03v9W1vTELs"},"source":["def knn_predict(x_new, X_train, y_train, k, distance_metric):\n","  distances = distance_metric(x_new, X_train)\n","  sorted_indices_by_distance = np.argsort(distances)\n","  nearest_k_neighbours_indices = sorted_indices_by_distance[:k]\n","  nearest_k_neighbours_classes = y_train[nearest_k_neighbours_indices]\n","  uniques, counts = np.unique(nearest_k_neighbours_classes, return_counts=True)\n","  yhat_new = uniques[np.argmax(counts)]\n","  return yhat_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can see the predictions for the test data:"],"metadata":{"id":"HK4yrHb2CV3O"}},{"cell_type":"code","metadata":{"id":"kaRxP_lXdydP"},"source":["m_test = X_test.shape[0]\n","for i in range(m_test):\n","  print(knn_predict(X_test[i, :], X_train, y_train, 3, L2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's use the *k*-NN classifier defined in scikit-learn!"],"metadata":{"id":"Hltihi7mBCEy"}},{"cell_type":"markdown","metadata":{"id":"haDtK-LSmDPc"},"source":["## With scikit-learn\n","\n","Read the documentation for the *k*-NN classifier implementation in scikit-learn (It's called the KNeighborsClassifier there and is inside the neighbors subpackage):\n","https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"]},{"cell_type":"code","metadata":{"id":"d0Mi3opfeBPe"},"source":["from  sklearn import neighbors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The *k* hyperparameter of *k*-NN is is called `n_neighbors`. We initialize a new object of the `KNeighborsClassifier` class. This object is the training algorithm , which also holds the produced ML model inside it (like all scikit-learn ML methods):"],"metadata":{"id":"Lc_1TRl-BY5h"}},{"cell_type":"code","metadata":{"id":"NgOazLXUmM3k"},"source":["knn = neighbors.KNeighborsClassifier(n_neighbors=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's use a very famous dataset, [the Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html):"],"metadata":{"id":"1u4I0ID9B76w"}},{"cell_type":"code","metadata":{"id":"M2jNEIDYoB4b"},"source":["data = datasets.load_iris()\n","X = data.data\n","y = data.target\n","df = pd.DataFrame(np.c_[X, y], \n","                  columns=np.append(data.feature_names, \"species\"))\n","df[\"species\"] = df[\"species\"].astype('uint8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXfzLDoO-QNS"},"source":["display(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, we split data into training and test sets, this time using a more reasonable ratio:"],"metadata":{"id":"ZAlywzCWCJ_z"}},{"cell_type":"code","metadata":{"id":"_RLaln2X-Qco"},"source":["X_train, X_test, y_train, y_test = \\\n","  model_selection.train_test_split(X, y, test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We train an ML model by the use of method `fit()`. This is again true for all ML methods in scikit-learn. We obviously have to feed in the (training) data:"],"metadata":{"id":"ElFUtn-3DLZB"}},{"cell_type":"code","metadata":{"id":"iQGLDLlJmToz"},"source":["knn.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This prints a summary of hyperparameters used afer the process of traning is done (which is trivial in the case of *k*-NN):\n","\n","Now, we can generate predictions on test data:"],"metadata":{"id":"y7vbFvJ4DaKl"}},{"cell_type":"code","metadata":{"id":"pMAIgXN4m62a"},"source":["knn.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's visualize how our *k*-NN classifier model is working. Let's use Plotly:"],"metadata":{"id":"nM-kgQyJDrnD"}},{"cell_type":"code","source":["import plotly.graph_objects as go"],"metadata":{"id":"AaynDEN0Dyu_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also, we really can't visualize 4 dimensions, so let's use only two of the featrues (the first two features) in the dataset and produce a 2D visualization:"],"metadata":{"id":"38CHbvk2DzFY"}},{"cell_type":"code","metadata":{"id":"cSyVuRm3pwyl"},"source":["X2_train = X_train[:, :2]\n","X2_test = X_test[:, :2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have to retrain a new model that uses only those two features:"],"metadata":{"id":"nhOYW8zQEEr_"}},{"cell_type":"code","metadata":{"id":"JfYoIw7Cp_7Z"},"source":["knn.fit(X2_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also visualize some test points. We need predictions for them to do that:"],"metadata":{"id":"KN5j_GLiFAat"}},{"cell_type":"code","metadata":{"id":"obEKNkIw4OaJ"},"source":["yhat_test = knn.predict(X2_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to calculate the ranges from our data to produce a good visualization:"],"metadata":{"id":"v9oSOz3wEJV6"}},{"cell_type":"code","metadata":{"id":"LQce7lkpm9E5"},"source":["x_mins = np.min(X, axis=0)\n","x_maxs = np.max(X, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's generate some points on a grid..."],"metadata":{"id":"Yp0rX2BhETJA"}},{"cell_type":"code","metadata":{"id":"702IeWZ4otkg"},"source":["x0_vis_range = np.arange(x_mins[0] - 0.1, x_maxs[0] + 0.1, 0.02)\n","x1_vis_range = np.arange(x_mins[1] - 0.1, x_maxs[1] + 0.1, 0.02)\n","XX0_vis, XX1_vis = np.meshgrid(x0_vis_range, x1_vis_range)\n","X_vis = np.c_[XX0_vis.flatten(), XX1_vis.flatten()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...produce predictions for them..."],"metadata":{"id":"Ql0ujMuhEqa4"}},{"cell_type":"code","metadata":{"id":"xP2zLP12pbpm"},"source":["yhat_vis = knn.predict(X_vis)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...reformat them into the required shape for visualization..."],"metadata":{"id":"gHGIXcNEEu02"}},{"cell_type":"code","metadata":{"id":"Fl6mIGfFpoYG"},"source":["YYhat_vis = yhat_vis.reshape(XX0_vis.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...and use that to visualize our *k*-NN classifier model:"],"metadata":{"id":"Ioua_VS7FMtP"}},{"cell_type":"code","metadata":{"id":"QrYxc2MAwt8S"},"source":["fig = go.Figure()\n","fig.add_trace(go.Heatmap(z=YYhat_vis, \n","                        x=x0_vis_range,\n","                        y=x1_vis_range,\n","                        showscale=False))\n","fig.add_trace(go.Scatter(x=X2_train[:, 0],\n","                        y=X2_train[:, 1],\n","                        mode='markers',\n","                        marker_color=y_train, \n","                        marker_line_width=1))\n","fig.add_trace(go.Scatter(x=X2_test[:, 0],\n","                        y=X2_test[:, 1],\n","                        mode='markers',\n","                        marker_color=yhat_test,\n","                        marker_line_width=3))\n","\n","fig.update_layout(showlegend=False)\n","fig.update_xaxes(range=[x_mins[0] - 0.1, x_maxs[0] + 0.1],\n","                title=data.feature_names[0])\n","fig.update_yaxes(range=[x_mins[1] - 0.1, x_maxs[1] + 0.1],\n","                title=data.feature_names[1])\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The coloured regions denote the prediction of our *k*-NN classifier for that region. The points with thick outlines are test points and the ones with thin outlines are training points."],"metadata":{"id":"NOkdbJeRFnmE"}},{"cell_type":"markdown","source":["Finally, let me show you an interactive visualizer which you can use to study the differnt *k*-NN classifier models that will be produced when you vary the value of *k*. Play with the slider and observe the results to complete your sense about the role of *k* in *k*-NN (you can expand the block to reveal the code underneath the visualization, but that is optional):"],"metadata":{"id":"Bv-i0u4-FOEi"}},{"cell_type":"code","metadata":{"id":"JeCP6snXxItw","cellView":"form"},"source":["#@title Interactive Visualizer: k { run: \"auto\" }\n","\n","k = 1 #@param {type:\"slider\", min:1, max:20, step:1}\n","\n","display(k)\n","\n","knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n","\n","knn.fit(X2_train, y_train)\n","\n","yhat_vis = knn.predict(X_vis)\n","YYhat_vis = yhat_vis.reshape(XX0_vis.shape)\n","\n","yhat_test = knn.predict(X2_test)\n","\n","fig = go.Figure()\n","fig.add_trace(go.Heatmap(z=YYhat_vis, \n","                        x=x0_vis_range,\n","                        y=x1_vis_range,\n","                        showscale=False))\n","fig.add_trace(go.Scatter(x=X2_train[:, 0],\n","                        y=X2_train[:, 1],\n","                        mode='markers',\n","                        marker_color=y_train, \n","                        marker_line_width=1))\n","fig.add_trace(go.Scatter(x=X2_test[:, 0],\n","                        y=X2_test[:, 1],\n","                        mode='markers',\n","                        marker_color=yhat_test,\n","                        marker_line_width=3))\n","fig.update_layout(showlegend=False)\n","fig.update_xaxes(range=[x_mins[0] - 0.1, x_maxs[0] + 0.1],\n","                title=data.feature_names[0])\n","fig.update_yaxes(range=[x_mins[1] - 0.1, x_maxs[1] + 0.1],\n","                title=data.feature_names[1])\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mIYBn_nM7TU"},"source":["That's all Folks!"]}]}